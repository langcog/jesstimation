---
title: "Zenith Abacus Estimation"
author: "Template for RMD from mosaic; most code from MCF, some from JS"
date: "June 8, 2015"
output: 
  html_document:
    fig_height: 8
    fig_width: 8
  pdf_document:
    fig_height: 7
    fig_width: 7
  word_document:
    fig_height: 7
    fig_width: 7
---

```{r include=FALSE}
# knitr settings to control how R chunks work.
require(knitr)
opts_chunk$set(
  cache=FALSE,
  size="small"    # slightly smaller font for code
)
```

Load  libraries
```{r, include = FALSE}
library(dplyr)
library(tidyr)
library(broom)
library(ggplot2)
library(lme4)
library(magrittr)
# require(mosaic)
rm(list=ls())
theme_set(theme_bw())
options(warn=-1)
```

Read in the data first, then combine Woodcock and Wiat and z-score the Standardized Test composite
```{r}
d <- read.csv("zenith all data.csv")
d$condition <- factor(d$condition, levels=c("control","abacus"))
d$standardized <- rowMeans(d[,c("wiat","woodcock")],na.rm=TRUE) 
```

# Descriptives and preliminaries

Summaries.
```{r}
d %>% 
  select(standardized, ans, deviance, linr2, ordinality, year) %>%
  group_by(year) %>% 
  do(tidy(summary(.))) %>% 
  data.frame
```

## Reliability for Estimation tasks

Deviance.
```{r}
data.frame(year=factor(c("1-2","2-3")),
           corrs=c(cor.test(d$deviance[d$year==1],
                            d$deviance[d$year==2])$estimate,
                   cor.test(d$deviance[d$year==2],
                            d$deviance[d$year==3])$estimate))


data.frame(year=factor(c("1-2","2-3")),
           pvals=c(cor.test(d$deviance[d$year==1],
                            d$deviance[d$year==2])$p.value,
                   cor.test(d$deviance[d$year==2],
                            d$deviance[d$year==3])$p.value))
```

Linear $r^2$.
```{r}
data.frame(year=factor(c("1-2","2-3")),
           corrs=c(cor.test(d$linr2[d$year==1],d$linr2[d$year==2])$estimate,
                   cor.test(d$linr2[d$year==2],d$linr2[d$year==3])$estimate))

data.frame(year=factor(c("1-2","2-3")),
          pvals=c(cor.test(d$linr2[d$year==1],d$linr2[d$year==2])$p.value,
                   cor.test(d$linr2[d$year==2],d$linr2[d$year==3])$p.value))
```


Ordinality.
```{r}
data.frame(year=factor(c("1-2","2-3")),
           corrs=c(cor.test(d$ordinality[d$year==1],
                            d$ordinality[d$year==2])$estimate,
                   cor.test(d$ordinality[d$year==2],
                            d$ordinality[d$year==3])$estimate))

data.frame(year=factor(c("1-2","2-3")),
          pvals=c(cor.test(d$ordinality[d$year==1],
                            d$ordinality[d$year==2])$p.value,
                   cor.test(d$ordinality[d$year==2],
                            d$ordinality[d$year==3])$p.value))
```

ANS.
```{r}
data.frame(year=factor(c("0-1","1-2","2-3")),
           corrs=c(cor.test(d$ans[d$year==0],
                            d$ans[d$year==1])$estimate, 
                   cor.test(d$ans[d$year==1],
                            d$ans[d$year==2])$estimate,
                   cor.test(d$ans[d$year==2],
                            d$ans[d$year==3])$estimate))

data.frame(year=factor(c("0-1","1-2","2-3")),
           pvals=c(cor.test(d$ans[d$year==0],
                            d$ans[d$year==1])$p.value, 
                   cor.test(d$ans[d$year==1],
                            d$ans[d$year==2])$p.value,
                   cor.test(d$ans[d$year==2],
                            d$ans[d$year==3])$p.value))
```
We see that the correlations are pretty decent year-to-year for our three DVs.

## Training effects

OK, now we test for effect of training for ANS. We can use a model b/c we have all 4 years:
```{r}
w.interaction <- lmer(ans ~ year  * condition + (subnum|year), data=d)
summary(w.interaction)
wo.interaction<-lmer(ans ~ year + condition + (subnum|year), data=d)
anova(w.interaction, wo.interaction)
```

Now, we do t-tests by task to find differences between control and abacus groups. We will correct for multiple comparisons post-hoc, correcting for the # of comparisons per DV. 

```{r}
tasks <- c("deviance","linr2","ans", "ordinality")

d %>% 
  filter(year > 0) %>%
  gather(measure, value, deviance, linr2, ans, ordinality) %>%
  group_by(year, measure) %>% 
  do(tidy(t.test(.$value[.$condition == "abacus"], 
                 .$value[.$condition == "control"])))
```

# PLOTS

ANS - Without intervention split
```{r, warnings=FALSE, message=FALSE}
qplot(ans, standardized, facets=~year, 
      data=d) + 
  geom_smooth(method="lm") +
  ylab("Standardized Test Composite") +
  xlab("Weber Fraction")        
```

And with:
```{r, warnings=FALSE, message=FALSE}
qplot(ans, standardized, facets=~year, 
      col=condition,
      data=d) + 
  geom_smooth(method="lm") +
  ylab("Standardized Test Composite") +
  xlab("Weber Fraction")        
```

Now do the same for deviance:
```{r, warnings=FALSE}
qplot(deviance, standardized, facets=~year,
      data=d) + 
  geom_smooth(method="lm") +
  ylab("Standardized Test Composite") +
  xlab("PAE")    

qplot(deviance, standardized, facets=~year, 
      col=condition,
      data=d) + 
  geom_smooth(method="lm") +
  ylab("Standardized Test Composite") +
  xlab("PAE")    
```

and for linear $r^2$:
```{r, warnings=FALSE}
qplot(linr2, standardized, facets=~year,
      data=d) + 
  geom_smooth(method="lm") +
  ylab("Standardized Test Composite") +
  xlab("Linear r^2")    

qplot(linr2, standardized, facets=~year, 
      col=condition,
      data=d) + 
  geom_smooth(method="lm") +
  ylab("Standardized Test Composite") +
  xlab("Linear r^2")       
```

and for ordinality:
```{r, warnings=FALSE}
qplot(ordinality, standardized, facets=~year,
      data=d) + 
  geom_smooth(method="lm") +
  ylab("Standardized Test Composite") +
  xlab("Ordinality")    

qplot(ordinality, standardized, facets=~year, 
      col=condition,
      data=d) + 
  geom_smooth(method="lm") +
  ylab("Standardized Test Composite") +
  xlab("Ordinality")       
```

# Models


Standardize predictors.
```{r}
# this is insane, but PCA will choke if the scaled variables have included in them the scaling attributes (and this is what R does by default). 
sscale <- function (x) {as.numeric(scale(x))}

md <- d %>% 
  gather(measure, value, deviance, linr2, ans, ordinality) %>%  
  group_by(year, measure) %>%
  mutate(standardized.scale = sscale(standardized),
         value.scale = sscale(value),
         wiat.scale = sscale(wiat), 
         woodcock.scale = sscale(woodcock),
         math.scale = sscale(math), 
         placeval.scale = sscale(placeval),
         arith.scale = sscale(arith), 
         mental.rot.scale = sscale(mental.rot),
         verbalwm.scale = sscale(verbalwm),
         spatialwm.scale = sscale(spatialwm),
         ravens.scale = sscale(ravens)) 
```

Now fit models. This uses the `broom` package - really very nifty.

NOTE, these are scaled values predicting scaled values. 
```{r}
std.baseline.models <- md %>%
  group_by(year, measure) %>%
  filter(!(year == 0 & measure != "ans")) %>%
  do(tidy(lm(standardized.scale ~ value.scale, data = .))) %>%
  filter(term != "(Intercept)") %>%
  data.frame
std.baseline.models
```

Check year 3 correlations

```{r}
cor.test(d$standardized[d$year==3],d$linr2[d$year==3])

cor.test(d$standardized[d$year==3],d$ans[d$year==3])

cor.test(d$standardized[d$year==3],d$deviance[d$year==3])

cor.test(d$standardized[d$year==3],d$ordinality[d$year==3])


```

And here are the full models.
```{r}
std.models <- md %>%
  group_by(year, measure) %>%
  filter(!(year == 0 & measure != "ans")) %>%
  do(tidy(lm(standardized.scale ~ value.scale + 
               mental.rot.scale + 
               verbalwm.scale + 
               spatialwm.scale + 
               ravens.scale + 
               age + condition, data = .)))

options(dplyr.width = Inf)
print.data.frame(std.models)

gelb<-as.numeric(std.models$p.value)
small<-subset(std.models, gelb<.05)
print.data.frame(small)




```

And make pretty plot. 
```{r}
std.models$term <- factor(std.models$term, 
                           levels = c("(Intercept)",
                                      "value.scale", "mental.rot.scale",
                                      "spatialwm.scale", "verbalwm.scale",
                                      "ravens.scale","age", "conditionabacus"), 
                           labels = c("Intercept", 
                                      "Predictor", "Mental Rotation",
                                      "Spatial WM", "Verbal WM",
                                      "Raven's", "Age", "Intervention"))

std.models$measure <- factor(std.models$measure,
                              levels = c("ans","deviance","linr2","ordinality"),
                              labels = c("ANS","PAE","Linear r^2", 
                                         "Ordinality"))
  
qplot(term, estimate, 
      fill = term, 
      ymin = estimate - std.error, ymax = estimate + std.error,
      geom = c("bar", "linerange"), stat = "identity", 
      facets = measure ~ year, 
      data=filter(std.models, term != "Intercept")) + 
  geom_hline(yintercept=0, lty=2) + 
  xlab("Predictor") + 
  ylab("Standardized Beta Weight") + 
  scale_fill_discrete(name="Predictor") +
  theme(axis.text.x = element_text(angle = 90, vjust=.5, hjust = 1)) 
```

Add PCA to the dataset. Note that PC1 is flipped just to make life easier.
```{r}
# need this to get around standard evals
pc1 <- function(x,y,z,m,n) {
  sscale(as.numeric(prcomp(~x + y + z + m + n)$x[,1]))
}

# need to filter to complete cases
pmd <- md %>%
  filter(complete.cases(wiat.scale, woodcock.scale, math.scale, 
                        placeval.scale, arith.scale)) %>%
  mutate(pc1 = -pc1(wiat.scale, woodcock.scale, math.scale, 
                   placeval.scale, arith.scale))
```

PCA Models and plot.
```{r}
pca.baseline.models <- pmd %>%
  group_by(year, measure) %>%
  filter(!(year == 0 & measure != "ans")) %>%
  do(tidy(lm(pc1 ~ value.scale, data = .))) %>%
  filter(term != "(Intercept)") %>%
  data.frame
pca.baseline.models


pca.models <- pmd %>%
  group_by(year, measure) %>%
  filter(!(year == 0 & measure != "ans")) %>%
  do(tidy(lm(pc1 ~ value.scale + 
               mental.rot.scale + 
               verbalwm.scale + 
               spatialwm.scale + 
               ravens.scale + 
               age + condition, data = .)))

options(dplyr.width = Inf)

print.data.frame(pca.models)


gelb2<-as.numeric(pca.models$p.value)
small2<-subset(pca.models, gelb2<.05)
print.data.frame(small2)

pca.models$term <- factor(pca.models$term, 
                          levels = c("(Intercept)",
                                     "value.scale", "mental.rot.scale",
                                     "spatialwm.scale", "verbalwm.scale",
                                     "ravens.scale","age", "conditionabacus"), 
                          labels = c("Intercept", 
                                     "Predictor", "Mental Rotation",
                                     "Spatial WM", "Verbal WM",
                                     "Raven's", "Age", "Intervention"))

pca.models$measure <- factor(pca.models$measure,
                              levels = c("ans","deviance","linr2","ordinality"),
                              labels = c("ANS","PAE","Linear r^2", 
                                         "Ordinality"))
  
qplot(term, estimate, 
      fill = term, 
      ymin = estimate - std.error, ymax = estimate + std.error,
      geom = c("bar", "linerange"), stat = "identity", 
      facets = measure ~ year, 
      data=filter(pca.models, term != "Intercept")) + 
  geom_hline(yintercept=0, lty=2) + 
  xlab("Predictor") + 
  ylab("Standardized Beta Weight") + 
  scale_fill_discrete(name="Predictor") +
  theme(axis.text.x = element_text(angle = 90, vjust=.5, hjust = 1)) 
```

## Convincing ourselves that the stats are OK

Now for the guts of one of the models, just an example from year 0, ANS. This uses ANOVA for model comparison. 
```{r}
## run models
# note, the filtering expression gets the model out of the same data frame that we used for the figure
# std.models <- md %>%
#   group_by(year, measure) %>%
#   filter(!(year == 0 & measure != "ans")) %>%
#   do(tidy(lm(standardized.scale ~ value.scale + 
#                mental.rot.scale + 
#                verbalwm.scale + 
#                spatialwm.scale + 
#                ravens.scale + 
#                age + condition, data = .)))

model1 <- lm(standardized.scale ~ value.scale + mental.rot.scale + verbalwm.scale + 
              spatialwm.scale + ravens.scale +  age + condition, 
             data = filter(md, year == 0, measure == "ans", 
                           complete.cases(value.scale)))
model2 <- lm(standardized.scale ~ mental.rot.scale + verbalwm.scale + 
              spatialwm.scale + ravens.scale +  age + condition, 
             data = filter(md, year == 0, measure == "ans", 
                           complete.cases(value.scale)))
summary(model1)
summary(model2)
anova(model1, model2)

# also note that p value for the anova is the same as the coefficient p value in the models data frame
filter(std.models, year == 0, measure == "ANS")
```

Year 0 Analyses by JS, to check betas
```{r, echo=FALSE}
#ensure year 0, ensure complete cases
y0 <- subset(d, year==0)
yc0 <- y0[complete.cases(y0[, c("ans","mental.rot","verbalwm","spatialwm","ravens")]),]


#scale all variables
yc0$standardized.scale <-scale(yc0$standardized)
yc0$ans.scale <-scale(yc0$ans)
yc0$mental.rot.scale<-scale(yc0$mental.rot)
yc0$age.scale<-scale(yc0$age)
yc0$verbalwm.scale<-scale(yc0$verbalwm)
yc0$spatialwm.scale<-scale(yc0$spatialwm)
yc0$ravens.scale<-scale(yc0$ravens)

#ANS, Year 0
withansyear0 <- lm(standardized.scale ~ ans.scale + mental.rot.scale + verbalwm.scale + 
              spatialwm.scale + ravens.scale +  age + condition, data = yc0)
withoutansyear0 <- lm(standardized.scale ~ mental.rot.scale + verbalwm.scale + 
              spatialwm.scale + ravens.scale +  age + condition, data = yc0)
summary(withansyear0)
summary(withoutansyear0)
anova(withansyear0, withoutansyear0)
```


Longitudial growth models
----

Let's start by plotting some growth curves for standardized math measures.

```{r}
qplot(year, standardized, facets=~subnum, 
      geom="line",
      data=subset(d, subnum %in% unique(subnum)[1:30])) + 
  ylim(c(0,1)) + 
  ylab("Standardized Math Tests (WIAT & WJ)") + 
  xlab("Year")
```

Now add some models. Check whether linear or quadratic is more useful. 

```{r}
preds <- c("standardized","year","subnum")
d.cc <- d[complete.cases(d[,preds]),preds]

mod1 <- lmer(standardized ~ year + (year | subnum), 
            data=d.cc)
mod2 <- lmer(standardized ~ year + I(year^2) + (year + I(year^2) | subnum), 
            data=d.cc)
anova(mod1,mod2)
```

No gain for this DV (standardized tests) in adding a quadratic term. Let's drop it then. 

Here's a basic model showing individaul variability plotted by the same model for all participants. It's a basic linear model and it doesn't do that badly. 

The approach we're going to follow is to try and see gains by adding predictors to this model to better capture individual variation. (Note that if we use individualized growth terms in a mixed model we already fit pretty much all the variance so there's not much else we can do, so we are using regular linear models here). 

```{r}
preds <- c("standardized","year","subnum")
d.cc <- d[complete.cases(d[,preds]),preds]

mod <- lm(standardized ~ year, 
            data=d.cc)
d.cc$mod <- predict(mod)

qplot(year, standardized, facets=~subnum, 
      geom="point",
      data=subset(d.cc, subnum %in% unique(subnum)[1:30])) + 
  geom_line(aes(x=year, y=mod),lty=2) + 
  ylim(c(0,1)) + 
  ylab("Standardized Math Tests (WIAT & WJ)") + 
  xlab("Year")
```

Let's try adding concurrent ANS as a predictor. As you can see, it does very little here if we have a single coefficient.

(Note that there are some missing values for ANS from year 0, so that's why curves are truncated). 

```{r}
preds <- c("standardized","year","subnum","ans")
d.cc <- d[complete.cases(d[,preds]),preds]

mod <- lm(standardized ~ year,
            data=d.cc)
mod2 <- lm(standardized ~ year + ans, 
            data=d.cc)
d.cc$mod <- predict(mod)
d.cc$mod2 <- predict(mod2)
anova(mod,mod2)

qplot(year, standardized, facets=~subnum, 
      geom="point",
      data=subset(d.cc, subnum %in% unique(subnum)[1:30])) + 
  geom_line(aes(x=year, y=mod),lty=2) + 
  geom_line(aes(x=year, y=mod2),lty=1,col="red") + 
  ylim(c(0,1)) + 
  ylab("Standardized Math Tests (WIAT & WJ)") + 
  xlab("Year")
```

Multiple coefficients, one for each year, and you start to see some teensy-tiny shape differences between curves. (I added abacus condition here as well). 

The dashed line is without ANS, the solid is with.

```{r}
preds <- c("standardized","year","subnum","condition","ans")
d.cc <- d[complete.cases(d[,preds]),preds]

mod <- lm(standardized ~ factor(year) * condition,
            data=d.cc)
mod2 <- lm(standardized ~ factor(year) * condition + 
             factor(year) * ans - ans, 
            data=d.cc)
d.cc$mod <- predict(mod)
d.cc$mod2 <- predict(mod2)
anova(mod,mod2)

qplot(year, standardized, facets=~subnum, 
      geom="point",col=condition,
      data=subset(d.cc, subnum %in% unique(subnum)[1:30])) + 
  geom_line(aes(x=year, y=mod),lty=2) + 
  geom_line(aes(x=year, y=mod2),lty=1) + 
  ylim(c(0,1)) + 
  ylab("Standardized Math Tests (WIAT & WJ)") + 
  xlab("Year")
```

Just for kicks, let's add everything into the mix and see how we do.

We are doing a bunch better in making customized predictions. In terms of significant effects, we get:


* arithmetic (big predictor of - you guessed it - standardized arithmetic) 
* abacus condition (whew)
* mental rotation
* raven's 
* verbal wm (but not spatial)

Nothing for ANS. Note that these predictions get noticeably worse when you leave out arithmetic scores. Domain knowledge is a big predictor. 

```{r}
preds <- c("standardized","year","subnum","condition","ans","mental.rot","spatialwm","verbalwm","ravens","age")
d.cc <- d[complete.cases(d[,preds]),preds]

mod <- lm(standardized ~ factor(year) * condition,
            data=d.cc)
mod2 <- lm(standardized ~ factor(year) + condition + 
             age + mental.rot + 
             spatialwm + ravens + verbalwm + ans, 
            data=d.cc)
d.cc$mod <- predict(mod)
d.cc$mod2 <- predict(mod2)

summary(mod2)

qplot(year, standardized, facets=~subnum, 
      geom="point",col=condition,
      data=subset(d.cc, subnum %in% unique(subnum)[1:30])) + 
  geom_line(aes(x=year, y=mod),lty=2) + 
  geom_line(aes(x=year, y=mod2),lty=1) + 
  ylim(c(0,1)) + 
  ylab("Standardized Math Tests (WIAT & WJ)") + 
  xlab("Year")
```


A final analysis: Let's try this by lagging the predictors. We use the year before to predict the current year - so this is trying to use e.g. year 0's ANS to see if it predicts growth over standard predictions in year 1.  

Note that I added gender to the baseline predictor set so we will see only lag differences. 

Define a function to do the lagging.

```{r}
lag <- function(x, p) {
  for (y in x$year) {
    if (y == 0) {
      eval(parse(text=paste("x$",p,".lag[x$year==0] <- 0",sep="")))
      } 
    
    if (y != 0 & ((y-1) %in% x$year)) {
      eval(parse(text=paste("x$",p,".lag[x$year==y] <- x$",
                            p,"[x$year==y-1]",sep="")))
      } else {
      eval(parse(text=paste("x$",p,".lag[x$year==y] <- NA",sep="")))      
      }
    }
  return(x)
  }
```

Now do the actual predictions. We are actually missing too many observations to really do this right without some serious imputation. We lose around a third of our data, as you can see from the plot. 

```{r, echo=FALSE}
# preds <- c("standardized","year","subnum","condition","ans","mental.rot",
#            "spatial.wm","verbalwm","ravens","age")
# pred_small <- c("ans","mental.rot","spatialwm","verbalwm","ravens","arith")
# d.cc <- d[complete.cases(d[,preds]),preds]
#   
# d.cc <- ddply(d.cc, .(subnum), 
#               function(x) {
#                 for (p in pred_small) {
#                   x <- lag(x,p)                  
#                   }
#                 return(x)
#                 })
# d.cc <- d.cc[complete.cases(d.cc),]
# 
# mod <- lm(standardized ~ factor(year) * condition + age,
#             data=d.cc)
# mod2 <- lm(standardized ~ factor(year) + condition + age + 
#               mental.rot.lag + 
#              spatialwm.lag + ravens.lag + verbalwm.lag + ans.lag, 
#             data=d.cc)
# d.cc$mod <- predict(mod)
# d.cc$mod2 <- predict(mod2)
# 
# summary(mod2)
# 
# qplot(year, standardized, facets=~subnum, 
#       geom="point",col=condition,
#       data=subset(d.cc, subnum %in% unique(subnum)[1:30])) + 
#   geom_line(aes(x=year, y=mod),lty=2) + 
#   geom_line(aes(x=year, y=mod2),lty=1) + 
#   ylim(c(0,1)) + 
#   ylab("Standardized Math Tests (WIAT & WJ)") + 
#   xlab("Year")
```

Because of the missing data problem, do this with just ANS, to see if anything else is different. Results:

* ANS is a predictor
* lagged ANS isn't
* None of this is visible in the plot because the differences are too small, so I left out the plot.
